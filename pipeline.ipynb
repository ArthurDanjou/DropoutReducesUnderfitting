{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dfd98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras import callbacks, layers, models, mixed_precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018dac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutScheduler(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Keras Callback to dynamically adjust the dropout rate.\n",
    "    Implements strategies from the paper 'Dropout Reduces Underfitting'.\n",
    "    Simplified version for dense networks (MLP).\n",
    "    \n",
    "    Args:\n",
    "        mode (str): 'standard', 'early', or 'late'.\n",
    "        switch_epoch (int): The epoch where the behavior switches.\n",
    "        rate (float): The dropout rate to apply when active (e.g., 0.2).\n",
    "        verbose (int): 1 to log changes, 0 for silence.\n",
    "        \n",
    "    Usage:\n",
    "        dropout_scheduler = DropoutScheduler(mode='early', switch_epoch=15, rate=0.2, verbose=1)\n",
    "        model.fit(X_train, y_train, epochs=50, callbacks=[dropout_scheduler])\n",
    "    \"\"\"\n",
    "    def __init__(self, mode: str, switch_epoch: int, rate: float, verbose=0):\n",
    "        super(DropoutScheduler, self).__init__()\n",
    "        # Validate mode\n",
    "        valid_modes = ['standard', 'early', 'late', 'none']\n",
    "        if mode not in valid_modes:\n",
    "            raise ValueError(f\"Mode must be one of {valid_modes}\")\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.switch_epoch = switch_epoch\n",
    "        self.active_rate = rate\n",
    "        self.verbose = verbose\n",
    "        self.dropout_layers = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Recursively retrieve all Dropout layers\n",
    "        self.dropout_layers = []\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, layers.Dropout):\n",
    "                self.dropout_layers.append(layer)\n",
    "            if hasattr(layer, 'layers'):\n",
    "                for sub_layer in layer.layers:\n",
    "                    if isinstance(sub_layer, layers.Dropout):\n",
    "                        self.dropout_layers.append(sub_layer)\n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(f\"[DropoutScheduler] {len(self.dropout_layers)} Dropout layers tracked.\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        new_rate = 0.0\n",
    "        \n",
    "        if self.mode == 'standard':\n",
    "            new_rate = self.active_rate\n",
    "        elif self.mode == 'none':\n",
    "            new_rate = 0.0\n",
    "        elif self.mode == 'early':\n",
    "            # Active at start, then disabled after switch_epoch\n",
    "            if epoch < self.switch_epoch:\n",
    "                new_rate = self.active_rate\n",
    "            else:\n",
    "                new_rate = 0.0     \n",
    "        elif self.mode == 'late':\n",
    "            # Inactive at start, then active after switch_epoch\n",
    "            if epoch < self.switch_epoch:\n",
    "                new_rate = 0.0\n",
    "            else:\n",
    "                new_rate = self.active_rate\n",
    "\n",
    "        # Apply rate\n",
    "        for layer in self.dropout_layers:\n",
    "            layer.rate = new_rate\n",
    "            \n",
    "        if self.verbose > 0 and epoch % 5 == 0:\n",
    "            status = \"ACTIVE\" if new_rate > 0 else \"INACTIVE\"\n",
    "            print(f\"[Epoch {epoch+1}] Strategy '{self.mode}': {status} (rate={new_rate:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331568f9",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "101e5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentPipeline:\n",
    "    \"\"\"Pipeline to run experiments with different datasets and models.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): 'mnist', 'cifar10', or 'cifar100'.\n",
    "        model_type (str): 'dense' or 'cnn'.\n",
    "        subset_fraction (float): Fraction of training data to use (0 < fraction <= 1.0).\n",
    "\n",
    "    Usage:\n",
    "        pipeline = ExperimentPipeline(dataset_name='mnist', model_type='dense', subset_fraction=0.5)\n",
    "        history = pipeline.train(mode='early', switch_epoch=15, rate=0.2, epochs=50)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset_name: str, model_type: str, subset_fraction: float = 1.0\n",
    "    ) -> None:\n",
    "        self.dataset_name = dataset_name\n",
    "        self.model_type = model_type\n",
    "        self.subset_fraction = subset_fraction\n",
    "        self._load_data(dataset_name, subset_fraction)\n",
    "        \n",
    "\n",
    "        # VÃ©rification de la dÃ©tection du GPU\n",
    "        gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "        if gpus:\n",
    "            print(f\"âœ… GPU detected : {len(gpus)} available(s)\")\n",
    "            for gpu in gpus:\n",
    "                print(\n",
    "                    f\"   -> Name : {tf.config.experimental.get_device_details(gpu)['device_name']}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"âš ï¸ NO GPU DETECTED. The code will run slowly on CPU.\")\n",
    "\n",
    "        if model_type == \"dense\":\n",
    "            self.model_factory = self._create_dense_model\n",
    "        elif model_type == \"cnn\":\n",
    "            self.model_factory = self._create_convolutional_model\n",
    "        else:\n",
    "            raise ValueError(f\"Model type '{model_type}' non reconnu.\")\n",
    "\n",
    "        print(f\"âœ… Pipeline Ready: {dataset_name.upper()} | {model_type.upper()}\")\n",
    "\n",
    "    def _load_data(self, name: str, subset_fraction: float = 1.0) -> None:\n",
    "        \"\"\"Load dataset and prepare train/val splits.\n",
    "\n",
    "        Args:\n",
    "            name (str): Dataset name.\n",
    "            subset_fraction (float): Fraction of training data to use.\n",
    "        \"\"\"\n",
    "        if name == \"mnist\":\n",
    "            (X_tr, y_tr), (X_te, y_te) = keras.datasets.mnist.load_data()\n",
    "            X_tr = np.expand_dims(X_tr, -1).astype(\"float32\") / 255.0\n",
    "            X_te = np.expand_dims(X_te, -1).astype(\"float32\") / 255.0\n",
    "        elif name == \"cifar10\":\n",
    "            (X_tr, y_tr), (X_te, y_te) = keras.datasets.cifar10.load_data()\n",
    "            X_tr = X_tr.astype(\"float32\") / 255.0\n",
    "            X_te = X_te.astype(\"float32\") / 255.0\n",
    "        elif name == \"cifar100\":\n",
    "            (X_tr, y_tr), (X_te, y_te) = keras.datasets.cifar100.load_data()\n",
    "            X_tr = X_tr.astype(\"float32\") / 255.0\n",
    "            X_te = X_te.astype(\"float32\") / 255.0\n",
    "        else:\n",
    "            raise ValueError(\"Dataset unknown\")\n",
    "\n",
    "        if subset_fraction < 1.0:\n",
    "            subset_size = int(len(X_tr) * subset_fraction)\n",
    "            X_tr = X_tr[:subset_size]\n",
    "            y_tr = y_tr[:subset_size]\n",
    "            print(\n",
    "                f\"âš ï¸ Using subset: {subset_fraction * 100:.1f}% of training data ({subset_size} samples)\"\n",
    "            )\n",
    "\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            X_tr, y_tr, test_size=0.2, random_state=42\n",
    "        )\n",
    "        self.input_shape = self.X_train.shape[1:]\n",
    "        self.num_classes = len(np.unique(self.y_train))\n",
    "\n",
    "    def _create_dense_model(self, dropout_rate: float) -> models.Sequential:\n",
    "        \"\"\"Create a simple dense MLP model with dropout.\n",
    "\n",
    "        Args:\n",
    "            dropout_rate (float): Dropout rate to use in Dropout layers.\n",
    "\n",
    "        Returns:\n",
    "            models.Sequential: Compiled Keras model.\n",
    "        \"\"\"\n",
    "        return models.Sequential(\n",
    "            [\n",
    "                layers.Input(shape=self.input_shape),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(256, activation=\"relu\"),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(128, activation=\"relu\"),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(64, activation=\"relu\"),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(self.num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _create_convolutional_model(self, dropout_rate: float) -> models.Sequential:\n",
    "        \"\"\"Create a simple CNN model with dropout.\n",
    "\n",
    "        Args:\n",
    "            dropout_rate (float): Dropout rate to use in Dropout layers.\n",
    "\n",
    "        Returns:\n",
    "            models.Sequential: Compiled Keras model.\n",
    "        \"\"\"\n",
    "        return models.Sequential(\n",
    "            [\n",
    "                layers.Input(shape=self.input_shape),\n",
    "                layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\"),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\"),\n",
    "                layers.MaxPooling2D(2),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(64, activation=\"relu\"),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(self.num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        mode: str,\n",
    "        switch_epoch: int,\n",
    "        rate: float,\n",
    "        epochs: int,\n",
    "        batch_size: int = 64,\n",
    "        verbose: int = 0,\n",
    "    ) -> keras.callbacks.History:\n",
    "        \"\"\"Train the model with specified dropout scheduling.\n",
    "\n",
    "        Args:\n",
    "            mode (str): 'standard', 'early', or 'late'.\n",
    "            switch_epoch (int): Epoch to switch dropout behavior.\n",
    "            rate (float): Dropout rate to use when active.\n",
    "            epochs (int): Number of training epochs.\n",
    "            batch_size (int): Batch size for training.\n",
    "            verbose (int): Verbosity level for training output.\n",
    "\n",
    "        Returns:\n",
    "            keras.callbacks.History: Training history object.\n",
    "        \"\"\"\n",
    "        keras.backend.clear_session()\n",
    "        model = self.model_factory(dropout_rate=rate)\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        scheduler = DropoutScheduler(\n",
    "            mode=mode, switch_epoch=switch_epoch, rate=rate, verbose=verbose\n",
    "        )\n",
    "        return model.fit(\n",
    "            self.X_train,\n",
    "            self.y_train,\n",
    "            validation_data=(self.X_val, self.y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[scheduler],\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    # --- COMPARISON PLOTS ---\n",
    "\n",
    "    def compare_learning_curves(\n",
    "        self,\n",
    "        modes: list = [\"standard\", \"early\", \"late\"],\n",
    "        switch_epoch: int = 10,\n",
    "        rate: float = 0.3,\n",
    "        epochs: int = 20,\n",
    "    ) -> None:\n",
    "        \"\"\"Train multiple modes and plot them on the same graph.\n",
    "\n",
    "        Args:\n",
    "            modes (list): List of modes to compare.\n",
    "            switch_epoch (int): Epoch to switch dropout behavior.\n",
    "            rate (float): Dropout rate to use when active.\n",
    "            epochs (int): Number of training epochs.\n",
    "        \"\"\"\n",
    "        colors = sns.color_palette(\"magma\", n_colors=len(modes))\n",
    "        histories = {}\n",
    "        print(\n",
    "            f\"\\nðŸ“Š Comparing Learning Curves: {modes} (Switch={switch_epoch}, Rate={rate})\"\n",
    "        )\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"   Running {mode}...\")\n",
    "            histories[mode] = self.train(\n",
    "                mode=mode, switch_epoch=switch_epoch, rate=rate, epochs=epochs\n",
    "            )\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        # Plot Train vs Val Accuracy\n",
    "        for mode, h in histories.items():\n",
    "            ax1.plot(h.history[\"val_accuracy\"], label=f\"{mode} (Val)\", linewidth=2, color=colors[modes.index(mode)])\n",
    "            ax1.plot(\n",
    "                h.history[\"accuracy\"],\n",
    "                label=f\"{mode} (Train)\",\n",
    "                linestyle=\"--\",\n",
    "                alpha=0.4,\n",
    "                color=colors[modes.index(mode)],\n",
    "            )\n",
    "\n",
    "        ax1.axvline(x=switch_epoch, color=\"gray\", linestyle=\":\", label=\"Switch\")\n",
    "        ax1.set_title(\"Validation Accuracy vs Epochs\")\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Accuracy\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot Train vs Val Loss\n",
    "        for mode, h in histories.items():\n",
    "            ax2.plot(h.history[\"val_loss\"], label=f\"{mode} (Val)\", linewidth=2, color=colors[modes.index(mode)])\n",
    "            ax2.plot(\n",
    "                h.history[\"loss\"], label=f\"{mode} (Train)\", linestyle=\"--\", alpha=0.4, color=colors[modes.index(mode)]\n",
    "            )\n",
    "\n",
    "        ax2.axvline(x=switch_epoch, color=\"gray\", linestyle=\":\", label=\"Switch\")\n",
    "        ax2.set_title(\"Validation Loss vs Epochs\")\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def compare_drop_rates(\n",
    "        self, rates: list, modes: list, switch_epoch: int, epochs: int\n",
    "    ) -> None:\n",
    "        \"\"\"Ablation: Accuracy vs Drop Rate (Train & Val gaps).\n",
    "\n",
    "        Args:\n",
    "            rates (list): List of dropout rates to test.\n",
    "            modes (list): List of modes to compare.\n",
    "            switch_epoch (int): Epoch to switch dropout behavior.\n",
    "            epochs (int): Number of training epochs.\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“Š Comparing Dropout Rates Impact on Accuracy\")\n",
    "        colors = sns.color_palette(\"magma\", n_colors=len(modes))\n",
    "        res_val = {mode: [] for mode in modes}\n",
    "        res_train = {mode: [] for mode in modes}\n",
    "\n",
    "        for rate in rates:\n",
    "            for mode in modes:\n",
    "                print(f\"   Running {mode} with rate={rate}...\")\n",
    "                h = self.train(\n",
    "                    mode=mode, switch_epoch=switch_epoch, rate=rate, epochs=epochs\n",
    "                )\n",
    "                res_val[mode].append(np.mean(h.history[\"val_accuracy\"][-3:]))\n",
    "                res_train[mode].append(np.mean(h.history[\"accuracy\"][-3:]))\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        colors = sns.color_palette(\"magma\", n_colors=len(modes))\n",
    "\n",
    "        for mode in modes:\n",
    "            plt.plot(\n",
    "                rates,\n",
    "                res_val[mode],\n",
    "                marker=\"o\",\n",
    "                label=f\"{mode} (Val)\",\n",
    "                color=colors[modes.index(mode)],\n",
    "                linewidth=2,\n",
    "            )\n",
    "            plt.plot(\n",
    "                rates,\n",
    "                res_train[mode],\n",
    "                marker=\"x\",\n",
    "                label=f\"{mode} (Train)\",\n",
    "                color=colors[modes.index(mode)],\n",
    "                linestyle=\"--\",\n",
    "                alpha=0.4,\n",
    "            )\n",
    "\n",
    "        plt.title(\"Accuracy vs Drop Rate (Train vs Val gap)\")\n",
    "        plt.xlabel(\"Dropout Rate\")\n",
    "        plt.ylabel(\"Final Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    def compare_switch_epochs(\n",
    "        self, switch_epochs: list, modes: list, rate: float, epochs: int\n",
    "    ) -> None:\n",
    "        print(\"\\nðŸ“Š Comparing Switch Epochs Impact on Accuracy\")\n",
    "        res_val = {mode: [] for mode in modes}\n",
    "        colors = sns.color_palette(\"magma\", n_colors=len(modes))\n",
    "\n",
    "        for s in switch_epochs:\n",
    "            for mode in modes:\n",
    "                print(f\"   Running {mode} with switch_epoch={s}...\")\n",
    "                h = self.train(mode=mode, switch_epoch=s, rate=rate, epochs=epochs)\n",
    "                res_val[mode].append(np.mean(h.history[\"val_accuracy\"][-3:]))\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for mode, accs in res_val.items():\n",
    "            plt.plot(switch_epochs, accs, marker=\"o\", label=mode, colors=colors[modes.index(mode)], linewidth=2)\n",
    "        plt.title(f\"Accuracy vs Switch Epoch (Rate={rate})\")\n",
    "        plt.xlabel(\"Switch Epoch\")\n",
    "        plt.ylabel(\"Final Validation Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "275dda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset_size_comparison(\n",
    "    dataset_name=\"cifar10\",\n",
    "    model_type=\"cnn\",\n",
    "    fractions=[0.1, 0.5, 1.0],\n",
    "    modes=[\"standard\", \"early\"],\n",
    "    rate=0.3,\n",
    "    switch_epoch=10,\n",
    "    epochs=20,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compare Early vs Standard dropout on variable dataset sizes.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Dataset to use ('mnist', 'cifar10', 'cifar100').\n",
    "        model_type (str): Model type ('dense' or 'cnn').\n",
    "        fractions (list): List of dataset size fractions to test.\n",
    "        modes (list): List of dropout modes to compare.\n",
    "        rate (float): Dropout rate to use when active.\n",
    "        switch_epoch (int): Epoch to switch dropout behavior.\n",
    "        epochs (int): Number of training epochs.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ“Š Comparing Dataset Sizes Impact on Performance\")\n",
    "\n",
    "    results = {mode: [] for mode in modes}\n",
    "\n",
    "    for frac in fractions:\n",
    "        print(f\"\\n>> Testing with {frac * 100}% of data...\")\n",
    "\n",
    "        pipe = ExperimentPipeline(\n",
    "            dataset_name, model_type, subset_fraction=frac,\n",
    "        )\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"   Running {mode}...\")\n",
    "            hist = pipe.train(\n",
    "                mode=mode,\n",
    "                switch_epoch=switch_epoch,\n",
    "                rate=rate,\n",
    "                epochs=epochs,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            final_acc = np.mean(hist.history[\"val_accuracy\"][-3:])\n",
    "            results[mode].append(final_acc)\n",
    "            print(f\"   -> {mode} Final Acc: {final_acc:.4f}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    markers = {\"standard\": \"s\", \"early\": \"o\", \"late\": \"^\"}\n",
    "\n",
    "    for mode, accs in results.items():\n",
    "        plt.plot(\n",
    "            fractions,\n",
    "            accs,\n",
    "            marker=markers.get(mode, \"o\"),\n",
    "            label=f\"{mode} Dropout\",\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "        )\n",
    "\n",
    "    plt.title(\n",
    "        f\"Impact of Dataset Size on Performance\\n({dataset_name} - {model_type} model)\"\n",
    "    )\n",
    "    plt.xlabel(\"Fraction of Training Data\")\n",
    "    plt.ylabel(\"Final Validation Accuracy\")\n",
    "    plt.xticks(fractions, [f\"{int(f * 100)}%\" for f in fractions])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c72a8b",
   "metadata": {},
   "source": [
    "## Experiment runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a65e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Comparing Dataset Sizes Impact on Performance\n",
      "\n",
      ">> Testing with 10.0% of data...\n",
      "âš ï¸ Using subset: 10.0% of training data (6000 samples)\n",
      "âœ… GPU detected : 1 available(s)\n",
      "   -> Name : Tesla T4\n",
      "âœ… Pipeline Ready: MNIST | DENSE\n",
      "   Running standard...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3696430561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m run_dataset_size_comparison(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mnist\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dense\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfractions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"standard\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"early\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-54707262.py\u001b[0m in \u001b[0;36mrun_dataset_size_comparison\u001b[0;34m(dataset_name, model_type, fractions, modes, rate, switch_epoch, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Running {mode}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             hist = pipe.train(\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mswitch_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswitch_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2968288983.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode, switch_epoch, rate, epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswitch_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswitch_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         return model.fit(\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m           )\n\u001b[1;32m    918\u001b[0m       )\n\u001b[0;32m--> 919\u001b[0;31m       return self._concrete_variable_creation_fn._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    920\u001b[0m           \u001b[0mfiltered_flat_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_variable_creation_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_dataset_size_comparison(\n",
    "    dataset_name=\"mnist\",\n",
    "    model_type=\"dense\",\n",
    "    fractions=[0.1, 0.3, 0.5, 1.0],\n",
    "    modes=[\"standard\", \"early\"],\n",
    "    rate=0.1,\n",
    "    switch_epoch=10,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7312eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU detected : 1 available(s)\n",
      "   -> Name : Tesla T4\n",
      "âœ… Pipeline Ready: CIFAR10 | CNN\n",
      "\n",
      "ðŸ“Š Comparing Learning Curves: ['standard', 'early', 'late'] (Switch=5, Rate=0.3)\n",
      "   Running standard...\n"
     ]
    }
   ],
   "source": [
    "exp = ExperimentPipeline(dataset_name=\"cifar10\", model_type=\"cnn\")\n",
    "\n",
    "# 1. Graphe 1: Courbes d'apprentissage superposÃ©es\n",
    "exp.compare_learning_curves(\n",
    "    modes=[\"standard\", \"early\", \"late\"], switch_epoch=5, rate=0.3, epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16905c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Graphe 2: Accuracy vs Drop Rate\n",
    "exp.compare_drop_rates(\n",
    "    rates=[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    modes=[\"standard\", \"early\", \"late\"],\n",
    "    epochs=15,\n",
    "    switch_epoch=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31613c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Graphe 3: Accuracy vs Epoch Switch\n",
    "exp.compare_switch_epochs(\n",
    "    switch_epochs=[2, 5, 10, 15],\n",
    "    modes=[\"early\", \"late\"],\n",
    "    epochs=15,\n",
    "    rate=0.3,\n",
    "    switch_epoch=5,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
