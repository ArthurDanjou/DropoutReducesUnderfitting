{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfd98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras import callbacks, layers, models, mixed_precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae34b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicDropout(layers.Layer):\n",
    "    \"\"\"Dropout layer with a dynamically adjustable dropout rate.\n",
    "\n",
    "    Args:\n",
    "        rate_variable: A TensorFlow variable representing the dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, rate_variable, **kwargs):\n",
    "        super(DynamicDropout, self).__init__(**kwargs)\n",
    "        self.rate_var = rate_variable\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate_var)\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018dac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutScheduler(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Keras Callback to dynamically adjust the dropout rate.\n",
    "    Implements strategies from the paper 'Dropout Reduces Underfitting'.\n",
    "    Simplified version for dense networks (MLP).\n",
    "    \n",
    "    Args:\n",
    "        mode (str): 'standard', 'early', or 'late'.\n",
    "        switch_epoch (int): The epoch where the behavior switches.\n",
    "        rate (float): The dropout rate to apply when active (e.g., 0.2).\n",
    "        verbose (int): 1 to log changes, 0 for silence.\n",
    "        \n",
    "    Usage:\n",
    "        dropout_scheduler = DropoutScheduler(mode='early', switch_epoch=15, rate=0.2, verbose=1)\n",
    "        model.fit(X_train, y_train, epochs=50, callbacks=[dropout_scheduler])\n",
    "    \"\"\" \n",
    "    def __init__(self, rate_variable, mode, switch_epoch, active_rate, verbose=0):\n",
    "        super(DropoutScheduler, self).__init__()\n",
    "        \n",
    "        valid_modes = [\"standard\", \"early\", \"late\", \"none\"]\n",
    "        if mode not in valid_modes:\n",
    "            raise ValueError(f\"Mode must be one of {valid_modes}\")\n",
    "        \n",
    "        self.rate_var = rate_variable\n",
    "        self.mode = mode\n",
    "        self.switch_epoch = switch_epoch\n",
    "        self.active_rate = active_rate\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.dropout_layers = []\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, layers.Dropout):\n",
    "                self.dropout_layers.append(layer)\n",
    "            if hasattr(layer, 'layers'):\n",
    "                for sub_layer in layer.layers:\n",
    "                    if isinstance(sub_layer, layers.Dropout):\n",
    "                        self.dropout_layers.append(sub_layer)\n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(f\"[DropoutScheduler] {len(self.dropout_layers)} Dropout layers tracked.\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        new_rate = 0.0\n",
    "\n",
    "        if self.mode == \"standard\":\n",
    "            new_rate = self.active_rate\n",
    "        elif self.mode == \"none\":\n",
    "            new_rate = 0.0\n",
    "        elif self.mode == \"early\":\n",
    "            if epoch < self.switch_epoch:\n",
    "                new_rate = self.active_rate\n",
    "            else:\n",
    "                new_rate = 0.0\n",
    "        elif self.mode == \"late\":\n",
    "            if epoch < self.switch_epoch:\n",
    "                new_rate = 0.0\n",
    "            else:\n",
    "                new_rate = self.active_rate\n",
    "\n",
    "        self.rate_var.assign(new_rate)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            status = \"ACTIF\" if new_rate > 0 else \"INACTIF\"\n",
    "            print(\n",
    "                f\"\\n[Epoch {epoch + 1}] Mode '{self.mode}': {status} (Taux={new_rate:.2f})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331568f9",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101e5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentPipeline:\n",
    "    \"\"\"Pipeline to run experiments with different datasets and models.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): 'mnist', 'cifar10', or 'cifar100'.\n",
    "        model_type (str): 'dense' or 'cnn'.\n",
    "        subset_fraction (float): Fraction of training data to use (0 < fraction <= 1.0).\n",
    "\n",
    "    Usage:\n",
    "        pipeline = ExperimentPipeline(dataset_name='mnist', model_type='dense', subset_fraction=0.5)\n",
    "        history = pipeline.train(mode='early', switch_epoch=15, rate=0.2, epochs=50)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, dataset_name: str, model_type: str, subset_fraction: float = 1.0\n",
    "    ):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.model_type = model_type\n",
    "        self.subset_fraction = subset_fraction\n",
    "        self.dropout_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
    "\n",
    "        self._load_data(dataset_name, subset_fraction)\n",
    "        self._check_gpu()\n",
    "\n",
    "        if model_type == \"dense\":\n",
    "            self.model_factory = self._create_dense_model\n",
    "        elif model_type == \"cnn\":\n",
    "            self.model_factory = self._create_convolutional_model\n",
    "        else:\n",
    "            raise ValueError(f\"Model type '{model_type}' non reconnu.\")\n",
    "\n",
    "        print(f\"âœ… Pipeline Ready: {dataset_name.upper()} | {model_type.upper()}\")\n",
    "\n",
    "    def _check_gpu(self):\n",
    "        \"\"\"Check for GPU availability and print status.\"\"\"\n",
    "        gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "        if gpus:\n",
    "            print(f\"âœ… GPU detected : {len(gpus)} available(s)\")\n",
    "        else:\n",
    "            print(\"âš ï¸ NO GPU DETECTED. Training might be slow.\")\n",
    "    \n",
    "    def _load_data(self, name: str, subset_fraction: float = 1.0) -> None:\n",
    "        \"\"\"Load dataset and prepare train/val splits.\n",
    "\n",
    "        Args:\n",
    "            name (str): Dataset name.\n",
    "            subset_fraction (float): Fraction of training data to use.\n",
    "        \"\"\"\n",
    "        if name == \"mnist\":\n",
    "            (X_tr, y_tr), (X_te, y_te) = keras.datasets.mnist.load_data()\n",
    "            X_tr = np.expand_dims(X_tr, -1).astype(\"float32\") / 255.0\n",
    "            X_te = np.expand_dims(X_te, -1).astype(\"float32\") / 255.0\n",
    "        elif name == \"cifar10\":\n",
    "            (X_tr, y_tr), (X_te, y_te) = keras.datasets.cifar10.load_data()\n",
    "            X_tr = X_tr.astype(\"float32\") / 255.0\n",
    "            X_te = X_te.astype(\"float32\") / 255.0\n",
    "        elif name == \"cifar100\":\n",
    "            (X_tr, y_tr), (X_te, y_te) = keras.datasets.cifar100.load_data()\n",
    "            X_tr = X_tr.astype(\"float32\") / 255.0\n",
    "            X_te = X_te.astype(\"float32\") / 255.0\n",
    "        elif name == \"fashion_mnist\":\n",
    "            (X_tr, y_tr), (X_te, y_te) = keras.datasets.fashion_mnist.load_data()\n",
    "            X_tr = np.expand_dims(X_tr, -1).astype(\"float32\") / 255.0\n",
    "            X_te = np.expand_dims(X_te, -1).astype(\"float32\") / 255.0\n",
    "        else:\n",
    "            raise ValueError(\"Dataset unknown\")\n",
    "\n",
    "        if subset_fraction < 1.0:\n",
    "            subset_size = int(len(X_tr) * subset_fraction)\n",
    "            X_tr = X_tr[:subset_size]\n",
    "            y_tr = y_tr[:subset_size]\n",
    "            print(\n",
    "                f\"âš ï¸ Using subset: {subset_fraction * 100:.1f}% of training data ({subset_size} samples)\"\n",
    "            )\n",
    "\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            X_tr, y_tr, test_size=0.2, random_state=42\n",
    "        )\n",
    "        self.input_shape = self.X_train.shape[1:]\n",
    "        self.num_classes = len(np.unique(self.y_train))\n",
    "\n",
    "    def _create_dense_model(self) -> models.Sequential:\n",
    "        \"\"\"Create a simple dense MLP model with dropout.\n",
    "\n",
    "        Args:\n",
    "            dropout_rate (float): Dropout rate to use in Dropout layers.\n",
    "\n",
    "        Returns:\n",
    "            models.Sequential: Compiled Keras model.\n",
    "        \"\"\"\n",
    "        return models.Sequential(\n",
    "            [\n",
    "                layers.Input(shape=self.input_shape),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(256, activation=\"relu\"),\n",
    "                DynamicDropout(self.dropout_var),\n",
    "                layers.Dense(128, activation=\"relu\"),\n",
    "                DynamicDropout(self.dropout_var),\n",
    "                layers.Dense(64, activation=\"relu\"),\n",
    "                DynamicDropout(self.dropout_var),\n",
    "                layers.Dense(self.num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _create_convolutional_model(self) -> models.Sequential:\n",
    "        \"\"\"Create a simple CNN model with dropout.\n",
    "\n",
    "        Args:\n",
    "            dropout_rate (float): Dropout rate to use in Dropout layers.\n",
    "\n",
    "        Returns:\n",
    "            models.Sequential: Compiled Keras model.\n",
    "        \"\"\"\n",
    "        return models.Sequential(\n",
    "            [\n",
    "                layers.Input(shape=self.input_shape),\n",
    "                layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\"),\n",
    "                DynamicDropout(self.dropout_var),\n",
    "                layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\"),\n",
    "                layers.MaxPooling2D(2),\n",
    "                DynamicDropout(self.dropout_var),\n",
    "                layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "                DynamicDropout(self.dropout_var),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(64, activation=\"relu\"),\n",
    "                DynamicDropout(self.dropout_var),\n",
    "                layers.Dense(self.num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        mode: str,\n",
    "        switch_epoch: int,\n",
    "        rate: float,\n",
    "        epochs: int,\n",
    "        batch_size: int = 64,\n",
    "        verbose: int = 0,\n",
    "    ) -> keras.callbacks.History:\n",
    "        \"\"\"Train the model with specified dropout scheduling.\n",
    "\n",
    "        Args:\n",
    "            mode (str): 'standard', 'early', or 'late'.\n",
    "            switch_epoch (int): Epoch to switch dropout behavior.\n",
    "            rate (float): Dropout rate to use when active.\n",
    "            epochs (int): Number of training epochs.\n",
    "            batch_size (int): Batch size for training.\n",
    "            verbose (int): Verbosity level for training output.\n",
    "\n",
    "        Returns:\n",
    "            keras.callbacks.History: Training history object.\n",
    "        \"\"\"\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        self.dropout_var.assign(0.0)\n",
    "\n",
    "        model = self.model_factory()\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        scheduler = DropoutScheduler(\n",
    "            rate_variable=self.dropout_var,\n",
    "            mode=mode,\n",
    "            switch_epoch=switch_epoch,\n",
    "            active_rate=rate,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        return model.fit(\n",
    "            self.X_train,\n",
    "            self.y_train,\n",
    "            validation_data=(self.X_val, self.y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[scheduler],\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def compare_learning_curves(\n",
    "        self,\n",
    "        modes: list = [\"standard\", \"early\", \"late\"],\n",
    "        switch_epoch: int = 10,\n",
    "        rate: float = 0.3,\n",
    "        epochs: int = 20,\n",
    "    ) -> None:\n",
    "        \"\"\"Train multiple modes and plot them on the same graph.\n",
    "\n",
    "        Args:\n",
    "            modes (list): List of modes to compare.\n",
    "            switch_epoch (int): Epoch to switch dropout behavior.\n",
    "            rate (float): Dropout rate to use when active.\n",
    "            epochs (int): Number of training epochs.\n",
    "        \"\"\"\n",
    "        colors = sns.color_palette(\"magma\", n_colors=len(modes))\n",
    "        histories = {}\n",
    "        print(\n",
    "            f\"\\nðŸ“Š Comparing Learning Curves: {modes} (Switch={switch_epoch}, Rate={rate})\"\n",
    "        )\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"   Running {mode}...\")\n",
    "            histories[mode] = self.train(\n",
    "                mode=mode, switch_epoch=switch_epoch, rate=rate, epochs=epochs\n",
    "            )\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        for mode, h in histories.items():\n",
    "            color = colors[modes.index(mode)]\n",
    "            ax1.plot(\n",
    "                h.history[\"val_accuracy\"],\n",
    "                label=f\"{mode} (Val)\",\n",
    "                linewidth=2,\n",
    "                color=color,\n",
    "            )\n",
    "            ax1.plot(\n",
    "                h.history[\"accuracy\"],\n",
    "                label=f\"{mode} (Train)\",\n",
    "                linestyle=\"--\",\n",
    "                alpha=0.4,\n",
    "                color=color,\n",
    "            )\n",
    "\n",
    "        ax1.axvline(x=switch_epoch, color=\"gray\", linestyle=\":\", label=\"Switch\")\n",
    "        ax1.set_title(\"Validation Accuracy vs Epochs\")\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Accuracy\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        for mode, h in histories.items():\n",
    "            color = colors[modes.index(mode)]\n",
    "            ax2.plot(\n",
    "                h.history[\"val_loss\"], label=f\"{mode} (Val)\", linewidth=2, color=color\n",
    "            )\n",
    "            ax2.plot(\n",
    "                h.history[\"loss\"],\n",
    "                label=f\"{mode} (Train)\",\n",
    "                linestyle=\"--\",\n",
    "                alpha=0.4,\n",
    "                color=color,\n",
    "            )\n",
    "\n",
    "        ax2.axvline(x=switch_epoch, color=\"gray\", linestyle=\":\", label=\"Switch\")\n",
    "        ax2.set_title(\"Validation Loss vs Epochs\")\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def compare_drop_rates(\n",
    "        self, rates: list, modes: list, switch_epoch: int, epochs: int\n",
    "    ) -> None:\n",
    "        \"\"\"Ablation: Accuracy vs Drop Rate (Train & Val gaps).\n",
    "\n",
    "        Args:\n",
    "            rates (list): List of dropout rates to test.\n",
    "            modes (list): List of modes to compare.\n",
    "            switch_epoch (int): Epoch to switch dropout behavior.\n",
    "            epochs (int): Number of training epochs.\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“Š Comparing Dropout Rates Impact (Accuracy & Loss)\")\n",
    "        colors = sns.color_palette(\"magma\", n_colors=len(modes))\n",
    "\n",
    "        res_acc = {mode: [] for mode in modes}\n",
    "        res_loss = {mode: [] for mode in modes}\n",
    "\n",
    "        for rate in rates:\n",
    "            for mode in modes:\n",
    "                print(f\"   Running {mode} with rate={rate}...\")\n",
    "                h = self.train(\n",
    "                    mode=mode, switch_epoch=switch_epoch, rate=rate, epochs=epochs\n",
    "                )\n",
    "\n",
    "                final_acc = np.mean(h.history[\"val_accuracy\"][-3:])\n",
    "                final_loss = np.mean(h.history[\"val_loss\"][-3:])\n",
    "\n",
    "                res_acc[mode].append(final_acc)\n",
    "                res_loss[mode].append(final_loss)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "        for mode in modes:\n",
    "            ax1.plot(\n",
    "                rates,\n",
    "                res_acc[mode],\n",
    "                marker=\"o\",\n",
    "                label=f\"{mode}\",\n",
    "                color=colors[modes.index(mode)],\n",
    "                linewidth=2,\n",
    "            )\n",
    "        ax1.set_title(\n",
    "            f\"Final Validation Accuracy vs Drop Rate\\n(Switch={switch_epoch})\"\n",
    "        )\n",
    "        ax1.set_xlabel(\"Dropout Rate\")\n",
    "        ax1.set_ylabel(\"Accuracy\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        for mode in modes:\n",
    "            ax2.plot(\n",
    "                rates,\n",
    "                res_loss[mode],\n",
    "                marker=\"s\",\n",
    "                label=f\"{mode}\",\n",
    "                color=colors[modes.index(mode)],\n",
    "                linewidth=2,\n",
    "                linestyle=\"--\",\n",
    "            )\n",
    "        ax2.set_title(f\"Final Validation Loss vs Drop Rate\\n(Switch={switch_epoch})\")\n",
    "        ax2.set_xlabel(\"Dropout Rate\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def compare_switch_epochs(\n",
    "        self, switch_epochs: list, modes: list, rate: float, epochs: int\n",
    "    ) -> None:\n",
    "        print(\"\\nðŸ“Š Comparing Switch Epochs Impact (Accuracy & Loss)\")\n",
    "        colors = sns.color_palette(\"magma\", n_colors=len(modes))\n",
    "\n",
    "        res_acc = {mode: [] for mode in modes}\n",
    "        res_loss = {mode: [] for mode in modes}\n",
    "\n",
    "        for s in switch_epochs:\n",
    "            for mode in modes:\n",
    "                print(f\"   Running {mode} with switch_epoch={s}...\")\n",
    "                h = self.train(mode=mode, switch_epoch=s, rate=rate, epochs=epochs)\n",
    "\n",
    "                final_acc = np.mean(h.history[\"val_accuracy\"][-3:])\n",
    "                final_loss = np.mean(h.history[\"val_loss\"][-3:])\n",
    "\n",
    "                res_acc[mode].append(final_acc)\n",
    "                res_loss[mode].append(final_loss)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        for mode in modes:\n",
    "            ax1.plot(\n",
    "                switch_epochs,\n",
    "                res_acc[mode],\n",
    "                marker=\"o\",\n",
    "                label=mode,\n",
    "                color=colors[modes.index(mode)],\n",
    "                linewidth=2,\n",
    "            )\n",
    "        ax1.set_title(f\"Final Val Accuracy vs Switch Epoch\\n(Rate={rate})\")\n",
    "        ax1.set_xlabel(\"Switch Epoch\")\n",
    "        ax1.set_ylabel(\"Accuracy\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        for mode in modes:\n",
    "            ax2.plot(\n",
    "                switch_epochs,\n",
    "                res_loss[mode],\n",
    "                marker=\"s\",\n",
    "                label=mode,\n",
    "                color=colors[modes.index(mode)],\n",
    "                linewidth=2,\n",
    "                linestyle=\"--\",\n",
    "            )\n",
    "        ax2.set_title(f\"Final Val Loss vs Switch Epoch\\n(Rate={rate})\")\n",
    "        ax2.set_xlabel(\"Switch Epoch\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "275dda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset_size_comparison(\n",
    "    dataset_name: str,\n",
    "    model_type: str,\n",
    "    fractions=[0.1, 0.5, 1.0],\n",
    "    modes=[\"standard\", \"early\"],\n",
    "    rate: float = 0.3,\n",
    "    switch_epoch: int =10,\n",
    "    epochs: int =20,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compare Early vs Standard dropout on variable dataset sizes.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Dataset to use ('mnist', 'cifar10', 'cifar100').\n",
    "        model_type (str): Model type ('dense' or 'cnn').\n",
    "        fractions (list): List of dataset size fractions to test.\n",
    "        modes (list): List of dropout modes to compare.\n",
    "        rate (float): Dropout rate to use when active.\n",
    "        switch_epoch (int): Epoch to switch dropout behavior.\n",
    "        epochs (int): Number of training epochs.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ“Š Comparing Dataset Sizes Impact (Accuracy & Loss)\")\n",
    "    results_acc = {mode: [] for mode in modes}\n",
    "    results_loss = {mode: [] for mode in modes}\n",
    "\n",
    "    for frac in fractions:\n",
    "        print(f\"\\n>> Testing with {frac * 100}% of data...\")\n",
    "\n",
    "        pipe = ExperimentPipeline(dataset_name, model_type, subset_fraction=frac)\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"   Running {mode}...\")\n",
    "            hist = pipe.train(\n",
    "                mode=mode,\n",
    "                switch_epoch=switch_epoch,\n",
    "                rate=rate,\n",
    "                epochs=epochs,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            final_acc = np.mean(hist.history[\"val_accuracy\"][-3:])\n",
    "            final_loss = np.mean(hist.history[\"val_loss\"][-3:])\n",
    "\n",
    "            results_acc[mode].append(final_acc)\n",
    "            results_loss[mode].append(final_loss)\n",
    "\n",
    "            print(f\"   -> {mode}: Acc={final_acc:.4f} | Loss={final_loss:.4f}\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    colors = sns.color_palette(\"magma\", n_colors=len(modes))\n",
    "    markers = {\"standard\": \"s\", \"early\": \"o\", \"late\": \"^\", \"none\": \"x\"}\n",
    "\n",
    "    for mode, accs in results_acc.items():\n",
    "        color = colors[modes.index(mode)]\n",
    "        marker = markers.get(mode, \"o\")\n",
    "        ax1.plot(\n",
    "            fractions,\n",
    "            accs,\n",
    "            marker=marker,\n",
    "            label=f\"{mode}\",\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "    ax1.set_title(\n",
    "        f\"Validation Accuracy vs Dataset Size\\n({dataset_name} - {model_type})\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Fraction of Training Data\")\n",
    "    ax1.set_ylabel(\"Final Accuracy\")\n",
    "    ax1.set_xticks(fractions)\n",
    "    ax1.set_xticklabels([f\"{int(f * 100)}%\" for f in fractions])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    for mode, losses in results_loss.items():\n",
    "        color = colors[modes.index(mode)]\n",
    "        marker = markers.get(mode, \"o\")\n",
    "        ax2.plot(\n",
    "            fractions,\n",
    "            losses,\n",
    "            marker=marker,\n",
    "            label=f\"{mode}\",\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "            color=color,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "\n",
    "    ax2.set_title(f\"Validation Loss vs Dataset Size\\n({dataset_name} - {model_type})\")\n",
    "    ax2.set_xlabel(\"Fraction of Training Data\")\n",
    "    ax2.set_ylabel(\"Final Loss\")\n",
    "    ax2.set_xticks(fractions)\n",
    "    ax2.set_xticklabels([f\"{int(f * 100)}%\" for f in fractions])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dropoutreducesunderfitting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
